---
layout: article
title: 神经网络结构:Graph Structure of Neural Networks
date: 2020-09-07 00:11:00 +0800
tags: [Deep Learning, GNN, Graph]
categories: blog
pageview: true
key: Graph-Structure-of-Neural-Networks
---

------

## 前言

神经网络通常被表示为神经元之间的连接图。然而，尽管它们被广泛使用，目前人们对**神经网络图结构**与其**预测性能**之间的关系知之甚少。本文系统地研究了神经网络的图结构是如何影响其预测性能的。为此，我们开发了一种新的基于图的神经网络表示，称为**关系图**，其中**神经网络的计算层对应于沿着图结构的几轮信息交换**。使用这种表示，我们表明：

1. 神经网络的图结构很重要；
2. 关系图的“甜蜜点”导致神经网络具有显著提高的预测性能；
3. 神经网络的性能近似是其关系图的**聚类系数**和**平均路径长度**的平滑函数；
4. 我们的发现在许多不同的任务和数据集中是一致的；
5. 可以有效地识别顶层结构；
6. 表现良好的神经网络具有与真实生物神经网络惊人地相似的图结构。

我们的工作为神经结构的设计和对神经网络的一般理解开辟了新的方向。



## 一、相关问题

- 网络结构与其预测性能之间是否有系统的联系?
- 性能良好的神经网络的结构特征是什么?
- 如何跨任务和数据集对这种结构签名进行基因化?
- 有没有一种有效的方法来检查一个给定的神经网络是否有前途?

研究难点：建立网络架构和精度之间的关系是困难的，因为神经网络和图之间的转化是未知的。自然的方式是用计算图来表示神经网络，但是这样做存在以下缺陷：

- 缺乏一般性:计算图受到所允许的图属性的限制，例如，这些图必须是有向的和无环的，层级之间是二部图，网络层面是单输入单输出。
- 与生物学/神经科学的不一致:生物神经网络的结构丰富得多，但不那么模式化。大脑网络中存在着信息交换，而不仅仅是单向流动。这样的生物或神经模型不能简单地用有向无环图来表示。

![image-20200908100040544](https://raw.githubusercontent.com/jjzhou012/image/master/blogImg20200908100040.png)

本文系统地研究了神经网络图结构与其预测性能之间的关系。开发了一种将神经网络表示为图的新方法，称之为**关系图**。主要关注**信息交换**，而不仅仅是定向数据流。

如图1所示：

- (a): 对于固定宽度的完全连接层，我们可以将一个输入通道和一个输出通道一起表示为单个节点，关系图中的一条边表示两个节点之间的信息交换;
- (b): 更多神经网络层和关系图的示例;
- (c): 根据关系图的图度量（包括平均路径长度和聚类系数）来探索关系图的设计空间，其中完全图对应于一个完全连通层;
- (d): 我们将这些关系图转换成神经网络，并研究它们的预测性能如何依赖于它们对应的关系图的图度量。

在此基础上，利用适当的信息交换定义，我们证明了关系图可以表示多种类型的神经网络层（完全连通层、卷积层等），同时摆脱了计算图的许多约束（如有向、无环、二部、单进单出）。一个神经网络层对应于关系图上的一轮信息交换，为了获得深度网络，我们在同一图上进行几轮信息交换。我们的新表示使我们能够建立更丰富、更多样化的神经网络，并使用成熟的网络科学工具对其进行分析。

然后，我们设计了一个名为 **WS-flex 的图形生成器**，它允许我们系统地探索神经网络的设计空间（即关系图）。基于神经科学的见解，我们通过关系图的聚类系数和平均路径长度来表征神经网络(c)。此外，我们的框架是灵活和通用的，因为我们可以将关系图转换为不同的神经体系结构，包括多层感知器（MLP）、卷积神经网络（CNNs）、ResNets等，并控制计算预算(d)。



## 二、关系图定义

### 2.1 图上的信息交换

我们首先从图的角度重新讨论神经网络的定义。

定义一个图为$$G=(\mathcal{V,E})$$。假设每个节点$$v$$有节点特征标量/向量$$x_v$$。

当$G$与神经元之间的信息交换有关时，我们称之为关系图。

具体来说，消息交换是由消息函数和聚合函数定义的，前者的输入是节点特性，输出是消息，后者的输入是一组消息，输出是更新后的节点特性。在每一轮消息交换中，每个节点向它的邻居发送消息，并聚合从它的邻居传入的消息。每条消息通过消息函数$$f(·)$$在每条边上进行转换，它们通过聚合函数$$AGG(\cdot)$$在每个节点上聚合。假设我们进行$R$轮信息交换，节点$v$的第$r$轮信息交换可以描述为：


$$
\mathbf{x}_{v}^{(r+1)}=\operatorname{AGG}^{(r)}\left(\left\{f_{v}^{(r)}\left(\mathbf{x}_{u}^{(r)}\right), \forall u \in N(v)\right\}\right)
$$


其中$$N(v)=\{u \mid v \vee(u, v) \in \mathcal{E}\}$$是节点$v$的邻居，且保证所有节点有自环。



![image-20200908114249477](https://raw.githubusercontent.com/jjzhou012/image/master/blogImg20200908114302.png)

我们在表1中总结了不同的实例。

![image-20200908132625611](https://raw.githubusercontent.com/jjzhou012/image/master/blogImg20200908132625.png)



### 2.2 固定宽度的MLPs

一个多层感知器(MLP)由多层叠加单元(神经元)组成，其中每个神经元执行标量输入和输出的加权和，然后是一些非线性运算。

假设一个MLP的第$r$层以$$\mathbf{x}^{(r)}$$为输入，$$\mathbf{x}^{(r+1)}$$为输出，则神经元的计算公式为：


$$
x_{i}^{(r+1)}=\sigma\left(\sum_{j} w_{i j}^{(r)} x_{j}^{(r)}\right)
$$
其中，$$w_{i j}^{(r)}$$是可训练的权重，$$x_{j}^{(r)}$$是输入$$\mathbf{x}^{(r)}$$的第$j$维（$$\mathbf{x}^{(r)}=\left(x_{1}^{(r)}, \ldots, x_{n}^{(r)}\right)$$），$$x_{i}^{(r+1)}$$是输出$$\mathbf{x}^{(r+1)}$$的第$i$维。

考虑极端情况下，当MLP的所有层的输入输出都有相同的特征维度时，一个全连接，固定宽度的MLP层可以表示为一个完整的关系图（全连接）。一个全连接固定宽度的MLP层有特殊的消息交换定义：$$f_{i}\left(x_{j}\right)=w_{i j} x_{j}$$，聚合函数为$$\operatorname{AGG}\left(\left\{x_{i}\right\}\right)=\sigma\left(\sum\left\{x_{i}\right\}\right)$$。

上面的讨论表明，可以将固定宽度的MLP视为具有特殊消息交换功能的完整关系图。因此，固定宽度的MLP是更为通用的模型系列中的一种特殊情况，其中消息函数、聚合函数，以及最重要的关系图结构可能会发生变化。

这使得我们可以将固定宽度的MLPs从使用完整关系图推广到任何基于一般关系图$$G$$。


$$
x_{i}^{(r+1)}=\sigma(\sum_{j \in N(i)} w_{i j}^{(r)} x_{j}^{(r)})
$$
其中$$i,j$$是图$$G$$中的节点。



### 2.3 一般的神经网络作为关系图

#### 2.3.1 可变宽度的MLP

对于一般的神经网络，一个重要的设计考虑是网络的层宽经常是不同的。例如，在CNNs中，常用的做法是在空间下采样后将层宽(特征通道数)增加一倍。为了表示层宽可变的神经网络，我们将节点特征从标量推广到向量：$$x_{i}^{(r)} \rightarrow \mathbf{x}_{i}^{(r)}$$，$$\mathbf{x}^{(r)}=\operatorname{CoNCAT}\left(\mathbf{x}_{1}^{(r)}, \ldots, \mathbf{x}_{n}^{(r)}\right)$$，同时将消息函数从标量乘法泛化到矩阵乘法。


$$
\mathbf{x}_{i}^{(r+1)}=\sigma(\sum_{j \in N(i)} \mathbf{W}_{i j}^{(r)} \mathbf{x}_{j}^{(r)})
$$


这里我们允许：

- 不同层中相同的节点可以有不同的维度：$$\mathbf{x}_{i}^{(r)}$$和$$\mathbf{x}_{i}^{(r+1)}$$；
- 同一层中不同节点可以有不同维度：$$\mathbf{x}_{i}^{(r)}$$和$$\mathbf{x}_{j}^{(r)}$$；